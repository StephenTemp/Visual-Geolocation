{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ee0d9a",
   "metadata": {},
   "source": [
    "# COMPSCI 682 – FINAL PROJECT\n",
    "### Problem: Conventional City Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db02cc3",
   "metadata": {},
   "source": [
    "Below **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cb53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# UTILS\n",
    "from vis_utils import visualize_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a14fa9",
   "metadata": {},
   "source": [
    "Below **Preproccess Data** \n",
    "\n",
    "\n",
    "**Step 0.1**: Preprocesses and reorganize the dataset\n",
    "\n",
    "The dataset images are denoted as follows (*from the Dataset website*):\n",
    "\n",
    "\n",
    "> *The name of the images has the following format: XXXXXX_Y.jpg. XXXXXX is the identifier of the placemark. There are total number of 10343 placemarks in this dataset, so XXXXXX ranges from 000001 to 10343.\n",
    "Y is the identifier of the view. 1, 2, 3 and 4 are the side views and 5 is the upward view. 0 is the view with markers overlaid (explained above). Thus, there are total number of 6 images per placemark.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdd06c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c7/qvwgmd795134ydlwhf04drnr0000gn/T/ipykernel_26065/2186100538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata_folders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpitts_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mny_folder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclass_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMacro_Classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclass_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_folders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mclass_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclass_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/Graduate/Semester 1 [Spring 2022]/COMPSCI 682 – Neural Networks/Final Project.nosync/src/utils/data_utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, folder_paths, img_wd, img_ht)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_ht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_wd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.data_utils import Macro_Classification\n",
    "\n",
    "\n",
    "pitts_folder = \"part1\"\n",
    "ny_folder = \"part5\"\n",
    "\n",
    "data_folders = [pitts_folder, ny_folder]\n",
    "class_data = Macro_Classification()\n",
    "class_data.load_data(folder_paths=data_folders)\n",
    "class_data.label_data()\n",
    "class_data.sample_dataset(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b6c62",
   "metadata": {},
   "source": [
    "**Step 1.** Preprocesses Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT numpy array to TENSOR\n",
    "X = class_data.data[0]\n",
    "y = class_data.labels\n",
    "\n",
    "print(\"num 0s [Pittsburgh]: \", np.sum(y == 0))\n",
    "print(\"num 1s [Orlando]: \", np.sum(y == 1))\n",
    "print(\"num 2s [New York]: \", np.sum(y == 2))\n",
    "print(\"data and labels equal: \", len(X) == len(y))\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e983fc",
   "metadata": {},
   "source": [
    "**Initialize** Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78abd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.torch_utils import process_data\n",
    "\n",
    "labels_to_exclude = [2]\n",
    "loader_train, loader_val = process_data(X, y, exclude_labels=labels_to_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e918e42",
   "metadata": {},
   "source": [
    "**Step 2.** Build Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_classifier import CNN_Classifier\n",
    "\n",
    "cnn_class = CNN_Classifier(num_classes=2, output_dims=2430)\n",
    "cnn_class.set_up_train(loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709bb51",
   "metadata": {},
   "source": [
    "**Step 3**. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dcc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(cnn_class.model.parameters(), lr=0.0008,\n",
    "                     momentum=0.9, nesterov=True)\n",
    "\n",
    "evals = cnn_class.train_model(optimizer, epochs=10, lr=0.0008, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7460946",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.plot(train_acc[:1000], label = \"train\")\n",
    "plt.plot(valid_acc[:1000], label = \"validation\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''\n",
    "import json\n",
    "\n",
    "with open(\"cnc_evals.json\", \"w\") as outfile:\n",
    "    json.dump(evals, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83475c",
   "metadata": {},
   "source": [
    "**Step 4.** Saliencey Maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c156c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; LongTensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make input tensor require gradient\n",
    "    X.requires_grad_()\n",
    "    \n",
    "    saliency = None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image. You first want to compute the loss over the correct   #\n",
    "    # scores (we'll combine losses across a batch by summing), and then compute  #\n",
    "    # the gradients with a backward pass.                                        #\n",
    "    ##############################################################################\n",
    "    # COMPUTE model scores \n",
    "    scores = model.forward(X)\n",
    "    \n",
    "    # COMPUTE the loss\n",
    "    rel_scores = scores.gather(1, y.view(-1, 1)).squeeze()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss = loss_func(scores, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # GET X derivative\n",
    "    X_derivs = X.grad\n",
    "    print(X_derivs.shape)\n",
    "    saliency, _ = torch.max(torch.abs(X_derivs), 1)\n",
    "    print(saliency.shape)\n",
    "        \n",
    "    # MAP instances to CORRECT label scores\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency\n",
    "\n",
    "def show_saliency_maps(X, y):\n",
    "    # Convert X and y from numpy arrays to Torch Tensors\n",
    "    X_tensor = torch.Tensor(X)\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "\n",
    "    # Compute saliency maps for images in X\n",
    "    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        image = np.moveaxis(X[i], (0, 1, 2), (2, 1, 0))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class_names = [\"New York\",\"Pittsburgh\", \"Orlando\"]\n",
    "\n",
    "\n",
    "def get_random_images(vis_data, num_images):\n",
    "    choices = np.zeros(shape=(num_images, 3, 100, 100))\n",
    "    choice_labels = np.zeros(shape=(num_images), dtype=int)\n",
    "    for i in range(num_images):\n",
    "        rand_index = random.randint(0, len(data))\n",
    "        X, y = data[rand_index] \n",
    "        choices[i] = torch.Tensor(X)\n",
    "        choice_labels[i] = y\n",
    "        \n",
    "    return choices, choice_labels\n",
    "    \n",
    "sample_X, sample_y = get_random_images(data, 4)\n",
    "print(sample_X.shape)\n",
    "      \n",
    "plt.figure(figsize=(100, 100))\n",
    "for i in range(len(sample_X)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    image = np.moveaxis(sample_X[i], (0, 1, 2), (2, 1, 0))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    plt.title(class_names[sample_y[i]])\n",
    "    plt.axis('off')\n",
    "plt.gcf().tight_layout()\n",
    "\n",
    "show_saliency_maps(sample_X, sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e7b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
